# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uGYF8ioCckdfSSwne6dmJc2gCELeQsmI
"""

import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm

# Load your recipe dataset
# Example columns: 'Title', 'Ingredients', 'Link'
df = pd.read_csv('recipes_processed1.csv')

# Preprocess: Combine Title + Ingredients for embedding
df['text'] = df['Title'] + '. Ingredients: ' + df['Core_Ingredients']
df['text2'] = df['Title'] + '. Ingredients: ' + df['Ingredients']

df['text'][0]

# Load pre-trained Sentence-BERT model
print("Loading Sentence-BERT model...")
model = SentenceTransformer('all-MiniLM-L6-v2')  # lightweight and fast

# Encode all recipe texts
print("Encoding recipe texts...")
recipe_texts = df['text'].tolist()
recipe_embeddings = model.encode(recipe_texts, show_progress_bar=True)

# Save embeddings to file (optional)
np.save('recipe_embeddings.npy', recipe_embeddings)

# Recommendation function
def recommend(user_ingredients, top_k=5):
    # Prepare user query
    user_text = 'Ingredients: ' + ', '.join(user_ingredients)

    # Encode user query
    user_embedding = model.encode([user_text])

    # Compute cosine similarities
    similarities = cosine_similarity(user_embedding, recipe_embeddings)[0]

    # Get top-k most similar recipes
    top_indices = similarities.argsort()[-top_k:][::-1]

    results = df.iloc[top_indices][['Title', 'Link', 'Core_Ingredients']]
    results['Similarity'] = similarities[top_indices]

    return results

# Example test usage
if __name__ == '__main__':
    print("\nExample recommendation:\n")
    user_ingredients = ['tomato', 'onion', 'garlic', 'basil']
    top_k = 5

    recommendations = recommend(user_ingredients, top_k=top_k)
    print(recommendations)

# Define manual test cases
test_cases = [
    (['paneer', 'tomato', 'onion'], 'Paneer Butter Masala'),
    (['egg', 'milk', 'flour'], 'Pancakes'),
    (['chicken', 'yogurt', 'garam masala'], 'Chicken Tikka'),
    (['rice', 'cumin', 'green peas'], 'Jeera Rice'),
    (['potato', 'mustard seeds', 'curry leaves'], 'Aloo Curry')
]

# Evaluation metric: Precision@1
correct = 0

for ingredients, expected_title in test_cases:
    print(f"\nIngredients: {ingredients}")
    recs = recommend(ingredients, top_k=1)
    top_title = recs.iloc[0]['Title']
    print(f"Top recommendation: {top_title}")

    if expected_title.lower() in top_title.lower():
        correct += 1

precision_at_1 = correct / len(test_cases)
print(f"\nOverall Precision@1: {precision_at_1:.2f}")

correct = 0
total = 50  # test on 50 random recipes

sample_df = df.sample(n=total, random_state=42)

for _, row in sample_df.iterrows():
    true_title = row['Title']
    ingredients = row['Core_Ingredients'].split(', ')

    recs = recommend(ingredients, top_k=5)

    top_titles = recs['Title'].tolist()

    if any(true_title.lower() in t.lower() for t in top_titles):
        correct += 1

precision_at_5 = correct / total
print(f'Precision@5 on {total} test cases: {precision_at_5:.2f}')